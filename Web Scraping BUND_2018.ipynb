{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d718f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b50e47",
   "metadata": {},
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d68dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "standings_url = \"https://fbref.com/en/comps/20/2017-2018/2017-2018-Bundesliga-Stats\"\n",
    "\n",
    "data = requests.get(standings_url)\n",
    "soup = BeautifulSoup(data.text)\n",
    "standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "links = [l for l in links if '/squads/' in l]\n",
    "team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "for team_url in team_urls:\n",
    "    team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "    data = requests.get(team_url)\n",
    "    matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
    "    data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "    shooting = pd.read_html(data.text, match=\"Shooting\")[0]\n",
    "    shooting.columns = shooting.columns.droplevel()\n",
    "    try:\n",
    "        team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]], on=\"Date\")\n",
    "    except ValueError:\n",
    "        continue\n",
    "    team_data = team_data[team_data[\"Comp\"] == \"Bundesliga\"]\n",
    "        \n",
    "    team_data[\"Season\"] = 2018\n",
    "    team_data[\"Team\"] = team_name\n",
    "    all_matches.append(team_data)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d07de",
   "metadata": {},
   "source": [
    "# to save score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f1cd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttable = pd.concat(all_matches)\n",
    "ttable=ttable.reset_index()\n",
    "ttable.drop('index',axis=1,inplace=True)\n",
    "ttable.to_csv(\"bun_2018_score&shoot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec455e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfdcf1ea",
   "metadata": {},
   "source": [
    "# assist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f66ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "standings_url = \"https://fbref.com/en/comps/20/2017-2018/2017-2018-Bundesliga-Stats\"\n",
    "\n",
    "data = requests.get(standings_url)\n",
    "soup = BeautifulSoup(data.text)\n",
    "standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "links = [l for l in links if '/squads/' in l]\n",
    "team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "for team_url in team_urls:\n",
    "    team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "    data = requests.get(team_url)\n",
    "    matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and 'all_comps/passing/' in l]\n",
    "    data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        \n",
    "    passing = pd.read_html(data.text, match=\"Passing\")[0]\n",
    "    passing.columns = passing.columns.droplevel()\n",
    "        \n",
    "    try:\n",
    "        team_data1 = matches.merge(passing[[\"Date\", \"KP\", \"1/3\", \"PPA\", \"CrsPA\"]], on=\"Date\")\n",
    "    except ValueError:\n",
    "        continue \n",
    "    team_data1 = team_data1[team_data1[\"Comp\"] == \"Bundesliga\"]\n",
    "        \n",
    "    team_data1[\"Season\"] = 2018\n",
    "    team_data1[\"Team\"] = team_name\n",
    "    all_matches.append(team_data1)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26b3e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "assists = pd.concat(all_matches)\n",
    "assists=assists.reset_index()\n",
    "assists.drop('index',axis=1,inplace=True)\n",
    "assists.to_csv(\"bun_2018_assist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67ee5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37e7ba30",
   "metadata": {},
   "source": [
    "# poss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c93e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "standings_url = \"https://fbref.com/en/comps/20/2017-2018/2017-2018-Bundesliga-Stats\"\n",
    "\n",
    "data = requests.get(standings_url)\n",
    "soup = BeautifulSoup(data.text)\n",
    "standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "links = [l for l in links if '/squads/' in l]\n",
    "team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "for team_url in team_urls:\n",
    "    team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "    data = requests.get(team_url)\n",
    "    matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and 'all_comps/possession/' in l]\n",
    "    data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        \n",
    "    possession = pd.read_html(data.text, match=\"Possession\")[0]\n",
    "    possession.columns = possession.columns.droplevel()\n",
    "        \n",
    "    try:\n",
    "        team_data2= matches.merge(possession[[\"Date\", \"Att 3rd\", \"Att Pen\"]], on=\"Date\")\n",
    "    except ValueError:\n",
    "        continue  \n",
    "    team_data2 = team_data2[team_data2[\"Comp\"] == \"Bundesliga\"]\n",
    "                \n",
    "    team_data2[\"Season\"] = 2018\n",
    "    team_data2[\"Team\"] = team_name\n",
    "    all_matches.append(team_data2)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4e6ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "posses = pd.concat(all_matches)\n",
    "posses=posses.reset_index()\n",
    "posses.drop('index',axis=1,inplace=True)\n",
    "posses.to_csv(\"bun_2018_posses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4590b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30bcc5b4",
   "metadata": {},
   "source": [
    "# GCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea32a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "standings_url = \"https://fbref.com/en/comps/20/2017-2018/2017-2018-Bundesliga-Stats\"\n",
    "\n",
    "data = requests.get(standings_url)\n",
    "soup = BeautifulSoup(data.text)\n",
    "standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "links = [l for l in links if '/squads/' in l]\n",
    "team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "for team_url in team_urls:\n",
    "    team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "    data = requests.get(team_url)\n",
    "    matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and 'all_comps/gca/' in l]\n",
    "    data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        \n",
    "    gfc = pd.read_html(data.text, match=\"Goal and Shot Creation\")[0]\n",
    "    gfc.columns = gfc.columns.droplevel()\n",
    "        \n",
    "    try:\n",
    "        team_data3 = matches.merge(gfc[[\"Date\", \"SCA\",\"GCA\"]], on=\"Date\")\n",
    "    except ValueError:\n",
    "        continue  \n",
    "    team_data3 = team_data3[team_data3[\"Comp\"] == \"Bundesliga\"]    \n",
    "        \n",
    "                \n",
    "    team_data3[\"Season\"] = 2018\n",
    "    team_data3[\"Team\"] = team_name\n",
    "    all_matches.append(team_data3)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d58b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "creation = pd.concat(all_matches)\n",
    "creation=creation.reset_index()\n",
    "creation.drop('index',axis=1,inplace=True)\n",
    "creation.to_csv(\"bun_2018_gca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b8d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08978ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccb425c",
   "metadata": {},
   "source": [
    "# back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfd50fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "standings_url = \"https://fbref.com/en/comps/20/2017-2018/2017-2018-Bundesliga-Stats\"\n",
    "\n",
    "data = requests.get(standings_url)\n",
    "soup = BeautifulSoup(data.text)\n",
    "standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "links = [l for l in links if '/squads/' in l]\n",
    "team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "for team_url in team_urls:\n",
    "    team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "    data = requests.get(team_url)\n",
    "    matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and 'all_comps/passing_types/' in l]\n",
    "    data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        \n",
    "    passing_type = pd.read_html(data.text, match=\"Pass Types\")[0]\n",
    "    passing_type.columns = passing_type.columns.droplevel()\n",
    "        \n",
    "    try:\n",
    "        team_data5 = matches.merge(passing_type[[\"Date\", \"TB\",\"CK\"]], on=\"Date\")\n",
    "    except ValueError:\n",
    "        continue\n",
    "    team_data5 = team_data5[team_data5[\"Comp\"] == \"Bundesliga\"]   \n",
    "        \n",
    "                \n",
    "    team_data5[\"Season\"] = 2018\n",
    "    team_data5[\"Team\"] = team_name\n",
    "    all_matches.append(team_data5)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f11e440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "back1=pd.concat(all_matches)\n",
    "back1=back1.reset_index()\n",
    "back1.drop('index',axis=1,inplace=True)\n",
    "back1.to_csv(\"bun_2018_back.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72835fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9398dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "668348cc",
   "metadata": {},
   "source": [
    "# misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "370a052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "standings_url = \"https://fbref.com/en/comps/20/2017-2018/2017-2018-Bundesliga-Stats\"\n",
    "\n",
    "data = requests.get(standings_url)\n",
    "soup = BeautifulSoup(data.text)\n",
    "standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "links = [l for l in links if '/squads/' in l]\n",
    "team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "    \n",
    "for team_url in team_urls:\n",
    "    team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "    data = requests.get(team_url)\n",
    "    matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "    links = [l for l in links if l and 'all_comps/misc/' in l]\n",
    "    data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        \n",
    "    passing_type = pd.read_html(data.text, match=\"Miscellaneous Stats\")[0]\n",
    "    passing_type.columns = passing_type.columns.droplevel()\n",
    "        \n",
    "    try:\n",
    "        team_data6 = matches.merge(passing_type[[\"Date\", \"CrdY\",\"CrdR\",\"2CrdY\",\"Fls\",\"Fld\",\"Off\",\"Crs\",\"Int\",\"TklW\",\"PKwon\",\"PKcon\",\"OG\"]], on=\"Date\")\n",
    "    except ValueError:\n",
    "        continue\n",
    "    team_data6 = team_data6[team_data6[\"Comp\"] == \"Bundesliga\"]   \n",
    "        \n",
    "                \n",
    "    team_data6[\"Season\"] = 2018\n",
    "    team_data6[\"Team\"] = team_name\n",
    "    all_matches.append(team_data6)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb984639",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc=pd.concat(all_matches)\n",
    "misc=misc.reset_index()\n",
    "misc.drop('index',axis=1,inplace=True)\n",
    "misc.to_csv(\"Bun_2018_misc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d02c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
